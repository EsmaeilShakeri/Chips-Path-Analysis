{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPvsy5ei4dX4"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "#### Collaborators Esmaeil Shakeri, Mahmoud Khalghollah\n",
        "#### Chips Analysis\n",
        "####Winter  2024\n",
        "\n",
        "**In this study we are going to detect the Chips path from recorded video that converted to png**\n",
        "\n",
        "####The following steps are summarized below:\n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIzuDRqljiCq"
      },
      "source": [
        "##1- Defining the libraries for the EfficientNetB7 classifcation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PSnjS3AUG_h",
        "outputId": "52521181-8937-419b-9d16-ab06740c97d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cleanlab\n",
            "  Downloading cleanlab-2.5.0-py3-none-any.whl (285 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.5/285.5 kB\u001b[0m \u001b[31m913.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from cleanlab) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from cleanlab) (1.2.2)\n",
            "Requirement already satisfied: tqdm>=4.53.0 in /usr/local/lib/python3.10/dist-packages (from cleanlab) (4.66.1)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from cleanlab) (1.5.3)\n",
            "Requirement already satisfied: termcolor>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cleanlab) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->cleanlab) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->cleanlab) (2023.3.post1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->cleanlab) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->cleanlab) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->cleanlab) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->cleanlab) (1.16.0)\n",
            "Installing collected packages: cleanlab\n",
            "Successfully installed cleanlab-2.5.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Need this to make sure there is no inconsistency\n",
        "from __future__ import print_function, absolute_import, division, with_statement\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "!pip install cleanlab\n",
        "\n",
        "import os\n",
        "from keras import regularizers\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "#from __future__ import print_function, absolute_import, division, with_statement\n",
        "import cleanlab\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "np.random.seed(477)\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
        "from keras.models import Model\n",
        "from keras import optimizers, applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
        "%matplotlib inline\n",
        "sns.set(style=\"whitegrid\")\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import tensorflow as tf\n",
        "import sklearn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0xNnusR_H4T",
        "outputId": "75fb5d46-c50f-4275-d607-156e8c483df8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DCVLO9IkJRS"
      },
      "source": [
        "##2- Imported two csv file from training and testing without noisy images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnSV_GbD_mVS"
      },
      "outputs": [],
      "source": [
        "# for test I removed some samples by random (not for main code)\n",
        "n = 1\n",
        "#test = pd.read_csv(\"/content/drive/MyDrive/CSV Original classification/test.csv\", skiprows=lambda i: i % n != 0)\n",
        "train = pd.read_csv(\".csv\", skiprows=lambda i: i % n != 0)\n",
        "train[\"pxl_nm\"] = train[\"pxl_nm\"].apply(lambda x: x + \".png\") #reading data form the folders\n",
        "#test[\"\"] = test[\"\"].apply(lambda x: x + \".png\")\n",
        "train['Chips'] = train['Chips'].astype('str')\n",
        "train['Chips'] = train['Chips'].apply(lambda x: x if x == '0' else '1') # to make the binary labels\n",
        "# test[''] = test[''].apply(lambda x: x if x == 0 else 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz_CgafckkrN"
      },
      "source": [
        "##3- Defining the Data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4TdkoMSM-U3"
      },
      "outputs": [],
      "source": [
        "# defining the parameters and creating the training generator\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 60\n",
        "WARMUP_EPOCHS = 2\n",
        "LEARNING_RATE = 1e-4\n",
        "WARMUP_LEARNING_RATE = 1e-3\n",
        "HEIGHT = 556 #please check the size of pixcel\n",
        "WIDTH = 556\n",
        "CANAL = 3\n",
        "N_CLASSES = train['Chips'].nunique()\n",
        "ES_PATIENCE = 5\n",
        "RLROP_PATIENCE = 3\n",
        "DECAY_DROP = 0.5\n",
        "X_train, X_val = train_test_split(train, test_size=0.2, random_state=160)\n",
        "train_datagen=ImageDataGenerator(rescale=1./255,\n",
        "                                 rotation_range=360,\n",
        "                                 horizontal_flip=True,\n",
        "                                 vertical_flip=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_generator=train_datagen.flow_from_dataframe(\n",
        "    dataframe=X_train,\n",
        "    directory=\"\",\n",
        "    x_col=\"pxl_nm\",\n",
        "    y_col=\"Chips\",\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    target_size=(HEIGHT, WIDTH),\n",
        "    seed=0)\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "valid_generator=validation_datagen.flow_from_dataframe(\n",
        "    dataframe=X_val,\n",
        "    directory=\"\",\n",
        "    x_col=\"pxl_nm\",\n",
        "    y_col=\"Chips\",\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    target_size=(HEIGHT, WIDTH),\n",
        "    seed=0)\n",
        "\n",
        "#test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "#test_generator = test_datagen.flow_from_dataframe(\n",
        "    #    dataframe=test,\n",
        "     #   directory = \"\",\n",
        "    #    x_col=\"\",\n",
        "   #     batch_size=1,\n",
        "   #     class_mode=None,\n",
        "   #     shuffle=False,\n",
        "  #      target_size=(HEIGHT, WIDTH),\n",
        "  #      seed=0)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baqmC7frU4cs"
      },
      "outputs": [],
      "source": [
        "#This is just for test to see the inline structure\n",
        "train_generator.reset()\n",
        "X,y = next(train_generator)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC9KXB_Me4Gi"
      },
      "source": [
        "##4- Implementing the EfficientNetB7 architecture  as a pre-train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGz8Y9L8NdMM"
      },
      "outputs": [],
      "source": [
        "def pred_model(input_shape, n_out):\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "    #base_model = tf.keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False,\n",
        "    base_model = tf.keras.applications.efficientnet.EfficientNetB7(weights='imagenet', include_top=False,\n",
        "                                       input_tensor=input_tensor)\n",
        "    # base_model.load_weights('drive/My Drive/Colab Notebooks/Esmaeil/data/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1048, kernel_regularizer=regularizers.l2(0.01), activation='relu')(x) #please check the total number of images\n",
        "    x = Dropout(0.5)(x)\n",
        "    final_output = Dense(n_out, activation='sigmoid', name='final_output')(x)\n",
        "    model = Model(input_tensor, final_output)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "model = pred_model(input_shape=(HEIGHT, WIDTH, CANAL), n_out=N_CLASSES)\n",
        "\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "for i in range(-5, 0):\n",
        "    model.layers[i].trainable = True\n",
        "\n",
        "#Defining class wiehgts for the multi-class issue.\n",
        "print(np.unique(train['Chips'].values))\n",
        "class_weights = sklearn.utils.class_weight.compute_class_weight('balanced', classes = np.unique(train['Chips'].astype('int').values), y=train['Chips'].astype('int').values)\n",
        "# class_weights = class_weight.compute_class_weight('balanced', classes = np.unique(train['diagnosis']), y = train['diagnosis'])\n",
        "class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "metric_list = [\"accuracy\"]\n",
        "optimizer = tf.keras.optimizers.Adam(lr=WARMUP_LEARNING_RATE)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy',  metrics=metric_list)\n",
        "model.summary()\n",
        "\n",
        "#earlystopping to make sure model does not work after perfirmace did not change\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "#es = EarlyStopping(monitor='val_loss', mode='min', patience=ES_PATIENCE, restore_best_weights=True, verbose=1)\n",
        "rlrop = ReduceLROnPlateau(monitor='val_loss', mode='min', patience=RLROP_PATIENCE, factor=DECAY_DROP, min_lr=1e-6, verbose=1)\n",
        "\n",
        "callback_list =[ rlrop] #[#es, rlrop]\n",
        "optimizer = tf.keras.optimizers.Adam(lr=LEARNING_RATE)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy',  metrics=metric_list)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKIdw2_WlRjo"
      },
      "source": [
        "##5- Training and testing the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WU4eTolBduV"
      },
      "outputs": [],
      "source": [
        "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
        "history_warmup = model.fit_generator(generator=train_generator,\n",
        "                                     steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                                     validation_data=valid_generator,\n",
        "                                     validation_steps=STEP_SIZE_VALID,\n",
        "                                     epochs=WARMUP_EPOCHS,\n",
        "                                     class_weight=class_weights,\n",
        "                                     verbose=1).history\n",
        "\n",
        "\n",
        "history_finetunning = model.fit_generator(generator=train_generator,\n",
        "                                          steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                                          validation_data=valid_generator,\n",
        "                                          validation_steps=STEP_SIZE_VALID,\n",
        "                                          epochs=EPOCHS,\n",
        "                                          callbacks=callback_list,\n",
        "                                          class_weight=class_weights,\n",
        "                                          verbose=1).history\n",
        "\n",
        "\n",
        "history = {'loss': history_warmup['loss'] + history_finetunning['loss'],\n",
        "           'val_loss': history_warmup['val_loss'] + history_finetunning['val_loss'],\n",
        "           'acc': history_warmup['acc'] + history_finetunning['acc'],\n",
        "           'val_acc': history_warmup['val_acc'] + history_finetunning['val_acc']}\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, sharex='col', figsize=(20, 14))\n",
        "\n",
        "ax1.plot(history['loss'], label='Train loss')\n",
        "ax1.plot(history['val_loss'], label='Validation loss')\n",
        "ax1.legend(loc='best')\n",
        "ax1.set_title('Loss')\n",
        "\n",
        "ax2.plot(history['acc'], label='Train acc')\n",
        "ax2.plot(history['val_acc'], label='Validation acc')\n",
        "ax2.legend(loc='best')\n",
        "ax2.set_title('Accuracy')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "sns.despine()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmzq2VkdISxI"
      },
      "outputs": [],
      "source": [
        "plt.plot(history['loss'])\n",
        "plt.plot(history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "\n",
        "plt.legend(['Train', 'Validation'], loc='upper left', bbox_to_anchor=(1,1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO_pq86EJBlu"
      },
      "outputs": [],
      "source": [
        "plt.plot(history['acc'])\n",
        "plt.plot(history['val_acc'])\n",
        "plt.title('Model AUC')\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left', bbox_to_anchor=(1,1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hK0GulaWeCw"
      },
      "source": [
        "##Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzYkg0kWTU1g"
      },
      "outputs": [],
      "source": [
        "# Create empty arays to keep the predictions and labels\n",
        "lastFullTrainPred = np.empty((0, N_CLASSES))\n",
        "lastFullTrainLabels = np.empty((0, N_CLASSES))\n",
        "lastFullValPred = np.empty((0, N_CLASSES))\n",
        "lastFullValLabels = np.empty((0, N_CLASSES))\n",
        "\n",
        "# Add train predictions and labels\n",
        "for i in range(STEP_SIZE_TRAIN+1):\n",
        "    im, lbl = next(train_generator)\n",
        "    scores = model.predict(im, batch_size=train_generator.batch_size)\n",
        "    lastFullTrainPred = np.append(lastFullTrainPred, scores, axis=0)\n",
        "    lastFullTrainLabels = np.append(lastFullTrainLabels, lbl, axis=0)\n",
        "\n",
        "# Add validation predictions and labels\n",
        "for i in range(STEP_SIZE_VALID+1):\n",
        "    im, lbl = next(valid_generator)\n",
        "    scores = model.predict(im, batch_size=valid_generator.batch_size)\n",
        "    lastFullValPred = np.append(lastFullValPred, scores, axis=0)\n",
        "    lastFullValLabels = np.append(lastFullValLabels, lbl, axis=0)\n",
        "\n",
        "\n",
        "lastFullComPred = np.concatenate((lastFullTrainPred, lastFullValPred))\n",
        "lastFullComLabels = np.concatenate((lastFullTrainLabels, lastFullValLabels))\n",
        "complete_labels = [np.argmax(label) for label in lastFullComLabels]\n",
        "\n",
        "train_preds = [np.argmax(pred) for pred in lastFullTrainPred]\n",
        "train_labels = [np.argmax(label) for label in lastFullTrainLabels]\n",
        "validation_preds = [np.argmax(pred) for pred in lastFullValPred]\n",
        "validation_labels = [np.argmax(label) for label in lastFullValLabels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Muo7AdaHWnyt"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(24, 7))\n",
        "labels = ['0 - No lesion', '1 - lesion']\n",
        "train_cnf_matrix = confusion_matrix(train_labels, train_preds)\n",
        "validation_cnf_matrix = confusion_matrix(validation_labels, validation_preds)\n",
        "\n",
        "train_cnf_matrix_norm = train_cnf_matrix.astype('float') / train_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "validation_cnf_matrix_norm = validation_cnf_matrix.astype('float') / validation_cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "train_df_cm = pd.DataFrame(train_cnf_matrix_norm, index=labels, columns=labels)\n",
        "validation_df_cm = pd.DataFrame(validation_cnf_matrix_norm, index=labels, columns=labels)\n",
        "\n",
        "sns.heatmap(train_df_cm, annot=True, fmt='.2f', cmap=\"Blues\", ax=ax1).set_title('Train')\n",
        "sns.heatmap(validation_df_cm, annot=True, fmt='.2f', cmap=sns.cubehelix_palette(8), ax=ax2).set_title('Validation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GbudhW9DYET"
      },
      "source": [
        "Sesitivity, Specifity, Percision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cohen_kappa_score"
      ],
      "metadata": {
        "id": "oCpgVmludrYz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}